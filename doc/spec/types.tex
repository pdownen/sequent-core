\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{braket}
\usepackage{hyperref}
\usepackage{ifthen}
\usepackage{preamble}

\begin{document}

\title{Typing Dual System FC and Sequent Core}

\maketitle

\section{Syntax}
\label{sec:syntax}

\begin{figure}[h]
\centering
\begin{gather*}
\begin{aligned}
  x, y, z, f, g, h, K &\in Var
  \\
  % q, r &\in KontVar
  % \\
  j &\in JumpVar
  \\
  a, b, T &\in TypeVar
  \\
  \cmd &\in Command
  &
  &::= \Let \abind \In c
  \Alt \braket{\tm | \vect[n]{\frm_n} | \ko}
  \Alt \jump{\vect[n]{\tm_n}}{j}
  \\
  \tm &\in Term
  &
  &::= x
  \Alt \fn{\ann x \ty} \tm
  \Alt \compute{\ann q \ty} \cmd
  \Alt \lit
  \Alt \ty
  \Alt \cn
  \\
  \ko &\in Kont
  &
  &::= \ret q
  \Alt \caseas{\ann x \ty}{\vect[n]{\analt_n}}
  % \Alt \vect[n]{\frm} \then \ko
  \\
  \frm &\in Frame
  &
  &::= \appto{\tm} \Alt \castby{\cn}
  \\
  \abind &\in Binding
  &
  &::= \nonrec \bp
  \Alt \rec{\vect[n]{\bp_n}}
  \\
  \bp &\in BindPair
  &
  &::= \bindv{\ann x \ty}{\tm}
  \Alt \bindk{\ann j \ty}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
  \\
  \analt &\in Alternative
  &
  &::= \alt{\blank}{\cmd}
  \Alt \alt{K ~ \vect[n]{\ann{x_n}{\ty_n}}}{\cmd}
  \Alt \alt{\lit}{\cmd}
  \\
  \ty &\in Type
  &
  &::= \dots
  \\
  \ki &\in Kind
  &
  &::= \dots
  \\
  \cn &\in Coercion
  &
  &::= \dots
  \\
  \decl &\in Declaration
  &
  &::= \dots
  \\
  \pgm &\in Program
  &
  &::= \decl; \pgm
  \Alt \cmd
\end{aligned}
\end{gather*}
\caption{Syntax of Dual System FC}
\label{fig:dual-fc-syntax}
\end{figure}

The syntax for Sequent Core is shown in Figure~\ref{fig:dual-fc-syntax}.  Types,
kinds, coercions, and declarations are unchanged by the sequent calculus
representation, so they are elided here.  Note that data constructors, written
$K$, are treated as a special sort of variable in the syntax, and additionally
type constructors, written $T$, are treated as a special sort of type variable.
To keep the similarity with Core, types are included in terms, but can only
appear in certain contexts.  In particular, we have the frame that instantiates
a polymorphic term by application to a type, $\inst{\ty}$, as well as binding a
type to a variable, $\bindv{a:\ki}{\ty}$.  In all other contexts, types as terms
are disallowed, like in the ill-formed term $\fn{\ann{x}{Bool}}Int$.

A note about the scope of bindings: terms can only reference value bindings
($\bindv{\ann{x}{\ty}}{\tm}$) and cannot reference continuation bindings
($\bindk{\ann{j}{\ty}}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}$).  There are some
consequences to the structure of $\Let$-bindings due to this scoping
restriction:
\begin{itemize}
\item Continuation bindings can reference value bindings and other continuation
  bindings, but value bindings can only reference other value bindings.
\item In any sequence of bindings, all value bindings can always be placed
  before all continuation bindings.
\item Value and continuation bindings cannot be mutually recursive.  Any
  minimal, mutually recursive $\rec{\vect[n]{\bp_n}}$ block will consist of only
  value bindings or only continuation bindings.
\end{itemize}
More details about scoping are discussed in Section~\ref{sec:scope-analysis}.

We use some conventional shorthands to make programs easier to read:
\begin{itemize}
\item If the type annotations on variables, \emph{ie} the $\ty$ in $\ann x \ty$,
  are not important to a particular example, we will often omit them.

% \item The function call constructor $\app{\blank}{\blank}$ associates to the
%   right, so $\app{1}{\app{2}{\app{3}{\ret q}}}$ is the same as
%   $\app{1}{(\app{2}{(\app{3}{\ret q})})}$.  Similarly, coercion continuations
%   associate to the right as well, so that
%   $\koerce{\cn_1}{\koerce{\cn_2}{\ret q}}$ is the same as
%   $\koerce{\cn_1}{(\koerce{\cn_2}{\ret q})}$.  Both function calls and coercions
%   share the same precedence and may be intermixed, so that
%   $\app{1}{\koerce{\cn_2}{\app{3}{\ret q}}}$ is the same as
%   $\app{1}{(\koerce{\cn_2}{(\app{3}{\ret q})})}$.

\item We will not always write the binding variable $\ann x \ty$ in the case
  continuation $\caseas{\ann x \ty}{\vect[n]{\analt_n}}$ when it turns out that
  $x$ is never referenced in $\alts$ or $\alts$, instead writing
  $\Case\vect[n]{\analt_n}$.  If instead $\ann x \ty$ is only referenced in the
  default alternative $\alt{\blank}{\cmd}$ in $\vect[n]{\analt_n}$, we will
  prefer to write $\ann x \ty$ in place of the wildcard $\blank$ pattern.  This
  often arises in a case continuation with \emph{only} a default alternative,
  $\caseas{\ann x \ty}{\alt{\blank}{\cmd}}$, which we write as the shortened
  $\Case\alt{\ann x \ty}{\cmd}$.

\item We will write the command $\braket{\tm | \mt | \ko}$ as just
  $\cut{\tm}{\ko}$.

\item
  % A sequence of frames, $\vect[n]{\frm_n}$, may be prepended onto a
  % continuation $\ko$, giving the new continuation $\vect[n]{\frm_n} \then \ko$
  % which transforms its input by each $\frm_n$ in turn before finally consuming
  % it with $\ko$.
  Sometimes it is handy to append a sequence of frames onto a term, as in
  $\tm ~ \vect[n]{\frm_n}$, which transforms the output of $\tm$ by each
  $\frm_n$ in turn before finally returning the result.  This form of term is
  defined as the macro-expansion in terms of a $\mu$-abstraction:
  \begin{align*}
    \tm ~ \vect[n]{\frm_n}
    &=
    \begin{cases}
      \tm & \text{if } \vect[n]{\frm_n} = \mt
      \\
      \comp{r}\braket{\tm | \vect[n]{\frm_n} | \ret r} & \text{otherwise}
    \end{cases}
  \end{align*}
\end{itemize}

\section{Scope and exit analysis}
\label{sec:scope-analysis}

\begin{figure}
\centering
\begin{gather*}
\begin{aligned}
  \Gamma \in& Environment
  &
  &::= \mt
  \Alt \Gamma, x
  \Alt \Gamma, a
  \\
  \Delta \in& KoEnvironment
  &
  &::= \mt
  \Alt \Delta, j
\end{aligned}
\end{gather*}

Term scoping: $\scopetm{\Gamma}{\tm}$
\begin{gather*}
  \infer
  {\scopetm{\Gamma}{x}}
  {x \in \Gamma}
  \qquad
  \axiom{\scopetm{\Gamma}{\lit}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\compute q \cmd}}
  {\scopecmd{\Gamma}{\mt}{\cmd}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\cn}}
  {\scopecn{\Gamma}{\cn}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\ty}}
  {\scopety{\Gamma}{\ty}}
  \\\\
  \infer
  {\scopetm{\Gamma}{\fn x \tm}}
  {\scopetm{\Gamma, x}{\tm}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\fn a \tm}}
  {\scopetm{\Gamma, a}{\tm}}
\end{gather*}

Continuation scoping: $\scopeko{\Gamma}{\Delta}{\ko}$
\begin{gather*}
  \axiom
  {\scopeko{\Gamma}{\Delta}{\ret q}}
  %{q \in \Delta}
  \qquad
  \infer
  {\scopeko{\Gamma}{\Delta}{\caseas{x}{\vect[n]{\analt_n}}}}
  {
    \vect[n]{
      \scopealt{\Gamma, x}{\Delta}{\analt_n}
    }
  }
  % \qquad
  % \infer
  % {\scopeko{\Gamma}{\Delta}{\vect[n]{\frm_n} \then \ko}}
  % {
  %   \scopefrm{\Gamma}{\vect[n]{\frm_n}}
  %   &
  %   \scopeko{\Gamma}{\Delta}{\ko}
  % }
\end{gather*}

Frame scoping: $\scopefrm{\Gamma}{\frm}$ and
$\scopefrm{\Gamma}{\vect[n]{\frm_n}}$
\begin{gather*}
  \axiom{\scopefrm{\Gamma}{\mt}}
  \qquad
  \infer
  {\scopefrm{\Gamma}{\frm' \then \vect[n]{\frm_n}}}
  {
    \scopefrm{\Gamma}{\frm'}
    &
    \scopefrm{\Gamma}{\vect[n]{\frm_n}}
  }
  \qquad
  \infer
  {\scopefrm{\Gamma}{\appto{\tm}}}
  {\scopetm{\Gamma}{\tm}}
  \qquad
  \infer
  {\scopefrm{\Gamma}{\castby{\cn}}}
  {\scopecn{\Gamma}{\cn}}
\end{gather*}

Command scoping: $\scopecmd{\Gamma}{\Delta}{\cmd}$
\begin{gather*}
  \infer
  {\scopecmd{\Gamma}{\Delta}{\Let \abind \In \cmd}}
  {
    \scopebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}
    &
    \scopecmd{\Gamma,\Gamma'}{\Delta,\Delta'}{\cmd}
  }
  \\\\
  \infer
  {\scopecmd{\Gamma}{\Delta}{\braket{\tm | \vect[n]{\frm_n} | \ko}}}
  {
    \scopetm{\Gamma}{\tm}
    &
    \scopefrm{\Gamma}{\vect[n]{\frm_n}}
    &
    \scopeko{\Gamma}{\Delta}{\ko}
  }
  \qquad
  \infer
  {\scopecmd{\Gamma}{\Delta}{\jump{\vect[n]{\tm_n}}{j}}}
  {
    j \in \Delta
    &
    \vect[n]{
      \scopetm{\Gamma}{\tm_n}
    }
  }
\end{gather*}


\emph{Further rules for $\scopety{\Gamma}{\ty}$, $\scopeki{\Gamma}{\ki}$, and
  $\scopecn{\Gamma}{\cn}$}
\caption{Scope and exit analysis for terms, continuations, and commands}
\label{fig:scoping-rules}
\end{figure}

\begin{figure}
\centering

Binding scoping: $\scopebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}$ and
$\scopebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}$
\begin{gather*}
  \infer
  {\scopebind{\Gamma}{\Delta}{\nonrec{\bp}}{\Gamma'}{\Delta'}}
  {\scopebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}}
  \qquad
  \infer
  {\scopebind{\Gamma}{\Delta}{\rec{\vect[n]{\bp_n}}}{\Gamma'}{\Delta'}}
  {
    \vect[n]{
      \scopebp{\Gamma,\Gamma'}{\Delta,\Delta'}{\bp_n}{\Gamma'_n}{\Delta'_n}
    }
    &
    \Gamma' = \vect[n]{\Gamma'_n}
    &
    \Delta' = \vect[n]{\Delta'_n}
  }
\end{gather*}
\begin{gather*}
  \infer
  {\scopebp{\Gamma}{\Delta}{\bindv{x}{\tm}}{x}{\mt}}
  {\scopetm{\Gamma}{\tm}}
  \qquad
  \infer
  {\scopebp{\Gamma}{\Delta}{\bindk{j}{\fnk{\vect[n]{x_n}}\cmd}}{\mt}{j}}
  {\scopecmd{\Gamma,\vect[n]{x_n}}{\Delta}{\cmd}}
\end{gather*}

Alternative scoping: $\scopealt{\Gamma}{\Delta}{\analt}$
\begin{gather*}
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{\blank}{\cmd}}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{\lit}{\cmd}}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{K ~ \vect[i]{x_i}}{\cmd}}}
  {\scopecmd{\Gamma,\vect[i]{x_i}}{\Delta}{\cmd}}
\end{gather*}

Program scoping: $\scopepgm{\Gamma}{\Delta}{\pgm}$
\begin{gather*}
  \infer
  {\scopepgm{\Gamma}{\Delta}{\decl; \pgm}}
  {
    \scopedecl{\Gamma}{\decl}{\Gamma'}
    &
    \scopepgm{\Gamma,\Gamma'}{\Delta}{c}
  }
  \qquad
  \infer
  {\scopepgm{\Gamma}{\Delta}{\cmd}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
\end{gather*}

\emph{Further rules for $\scopedecl{\Gamma}{\decl}{\Gamma'}$.}
\caption{Scope and exit analysis for bindings, alternatives, and
  programs}
\label{fig:scoping-rules-binds}
\end{figure}

The scoping rules for variables are shown in Figures~\ref{fig:scoping-rules} and
\ref{fig:scoping-rules-binds}, where the rules for scoping inside types, kinds,
coercions, and declarations are elided.  Continuation and jump variables are
treated differently from the other sorts of variables, being placed in a
separate environment $\Delta$, in order to prevent non-functional uses of
control flow.

Besides the normal rules for checking variable scope, these rules effectively
also perform an \emph{exit analysis} on a program (bindings, terms, commands,
\emph{etc}).  The one major restriction that we enforce is that terms must
always have a \emph{unique} exit point and cannot jump outside their scope.  The
intuition is:
\begin{quote}
  Terms cannot contain any references to free continuation variables.
\end{quote}
This restriction makes sure that $\lambda$-abstractions cannot close over
continuation variables available from its context, so that let-bound
parameterized continuations do not escape through a returned
$\lambda$-abstraction.  Thus, all the jump variables used within a
$\lambda$-abstraction must be local to that $\lambda$-abstraction.
Additionally, in all computations $\comp{r}\cmd$, the underlying command $c$ has
precisely one unique exit point, denoted by $\ret r$, which gives the returned
result of the computation.  Any parameterized continuations referenced inside of
$c$ must be local to the term itself, and not refer to its enclosing scope.

% If the command $c$ inside the well-scoped term $\comp{r}\cmd$ stops execution
% with some value $V$ sent to some continuation variable $q$, then we know that:
% \begin{itemize}
% \item $q$ must be equal to $r$, due to the fact that $r$ is the only allowable
%   free continuation variable inside of $\cmd$, and
% \item $r$ does not appear free inside the resulting value $V$, again due to the
%   scoping rules for continuation variables inside of a command.
% \end{itemize}
% In the simple case, this means execution of the term $\comp{r}\cmd$ yields
% $\comp{r}\cut{V}{\ret r}$, which $\eta$-reduces to just the value $V$ by the
% previously mentioned reasoning.  Thus, evaluating a term always results in a
% unique value.

Notice that these scoping rules, while not very complex, still manage to tell us
something about the expressive capabilities of the language.  For example, we
syntactically permit value and continuation bindings within the same recursive
block, but can they mutually call one another?  It turns out that these scoping
rules disallow any sort of interesting mutual recursion between terms and
continuations because terms are \emph{prevented} from referencing continuations
within their surrounding (or same) binding environment.

For example, in a simple case where we have the recursive bindings:
\begin{gather*}
  \rec{\bindv{f}{\fn x \tm}; \bindk{j}{\fnk{y}\cmd}}
\end{gather*}
then by the scoping rules, $q$ may call $f$ through $\cmd$, but $f$ cannot jump
back to $j$ in $\tm$ because $\fn x tm$ cannot contain the free reference to
$j$.  Therefore, since there is no true mutual recursion between both $f$ and
$j$, we can break the recursive bindings into two separate blocks with the
correct scope:
\begin{gather*}
  \rec{\bindv{f}{\fn x \tm}}; \rec{\bindk{j}{\fnk{y}\cmd}}
\end{gather*}
While we do not syntactically enforce this restriction, it would not case any
loss of expressiveness.  Indeed, we could further the partitions into
\begin{enumerate}
\item first, the list of value bindings, and
\item second, the list of continuation bindings,
\end{enumerate}
since continuations can refer to previously bound terms but not vice versa.
However, we do not make this distinction here.

\section{Type checking}
\label{sec:typing}

\begin{figure}
\centering
\begin{align*}
  \Gamma \in& Environment
  &
  &::= \mt
  \Alt \Gamma, x : \ty
  \Alt \Gamma, a : \ki
  \\
  \Delta \in& KoEnvironment
  &
  &::= \ret q : \ty
  \Alt \Delta, j : \ty
\end{align*}

Term typing: $\typetm{\Gamma}{\tm}{\ty}$
\begin{gather*}
  \infer
  {\typetm{\Gamma}{x}{\ty}}
  {x:\ty \in \Gamma}
  \qquad
  \infer
  {\typetm{\Gamma}{\lit}{\ty}}
  {\ty = literalType(\lit)}
  \qquad
  \infer
  {\typetm{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}}
  {\typecn{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}}
  \qquad
  \infer
  {\typetm{\Gamma}{\ty}{\ki}}
  {\typety{\Gamma}{\ty}{\ki}}
  \\\\
  \infer
  {\typetm{\Gamma}{\compute{\ann q \ty} \cmd}{\ty}}
  {\typecmd{\Gamma}{\ret q:\ty}{\cmd}}
  \qquad
  \infer
  {\typetm{\Gamma}{\fn{\ann x {\ty_1}} \tm}{\ty_1 \to \ty_2}}
  {\typetm{\Gamma, x:\ty_1}{\tm}{\ty_2}}
  \qquad
  \infer
  {\typetm{\Gamma}{\fn{\ann a \ki} \tm}{\forall \ann a \ki. \ty}}
  {\typetm{\Gamma, a:\ki}{\tm}{\ty}}
\end{gather*}

Continuation typing: $\typeko{\Gamma}{\Delta}{\ko}{\ty}$
\begin{gather*}
  \infer
  {\typeko{\Gamma}{\Delta}{\ret q}{\ty}}
  {\ret q:\ty \in \Delta}
  \qquad
  \infer
  {\typeko{\Gamma}{\Delta}{\caseas{\ann x \ty}{\vect[n]{\analt_n}}}{\ty}}
  {
    \vect[n]{
      \typealt{\Gamma, x:\ty}{\Delta}{\analt_n}{\ty}
    }
  }
  % \\\\
  % \infer
  % {\typeko{\Gamma}{\Delta}{\vect[n]{\frm_n} \then \ko}{\ty}}
  % {
  %   \typefrm{\Gamma}{\vect[n]{\frm_n}}{\ty}{\ty'}
  %   &
  %   \typeko{\Gamma}{\Delta}{\ko}{\ty'}
  % }
\end{gather*}

Frame typing: $\typefrm{\Gamma}{\frm}{\ty}{\ty'}$ and
$\typefrm{\Gamma}{\vect[n]{\frm_n}}{\ty}{\ty'}$
\begin{gather*}
  \axiom{\typefrm{\Gamma}{\mt}{\ty}{\ty}}
  \qquad
  \infer
  {\typefrm{\Gamma}{\frm' \then \vect[n]{\frm_n}}{\ty_1}{\ty_3}}
  {
    \typefrm{\Gamma}{\frm'}{\ty_1}{\ty_2}
    &
    \typefrm{\Gamma}{\vect[n]{\frm_n}}{\ty_2}{\ty_3}
  }
\end{gather*}
\begin{gather*}
  \infer
  {\typefrm{\Gamma}{\appto{\tm}}{(\ty \to \ty')}{\ty'}}
  {\typetm{\Gamma}{\tm}{\ty}}
  \qquad
  \infer
  {\typefrm{\Gamma}{\inst{\ty}}{(\forall a:\ki.\ty')}{\ty'\subst{a}{\ty}}}
  {\typety{\Gamma}{\ty}{\ki}}
  \\\\
  \infer
  {\typefrm{\Gamma}{\castby{\cn}}{\ty}{\ty'}}
  {\typecn{\Gamma}{\cn}{\eqty{\ty}{\ty'}}}
\end{gather*}

Command typing: $\typecmd{\Gamma}{\Delta}{\cmd}$
\begin{gather*}
  \infer
  {\typecmd{\Gamma}{\Delta}{\Let \abind \In \cmd}}
  {
    \typebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}
    &
    \typecmd{\Gamma,\Gamma'}{\Delta,\Delta'}{\cmd}
  }
  \\\\
  \infer
  {\typecmd{\Gamma}{\Delta}{\braket{\tm | \vect[n]{\frm_n} | \ko}}}
  {
    \typetm{\Gamma}{\tm}{\ty}
    &
    \typefrm{\Gamma}{\vect[n]{\frm}}{\ty}{\ty'}
    &
    \typeko{\Gamma}{\Delta}{\ko}{\ty'}
  }
  \qquad
  \infer
  {\typecmd{\Gamma}{\Delta}{\jump{\vect[n]{\tm_n}}{j}}}
  {
    j:\ty \in \Delta
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}
  }
\end{gather*}

\emph{Further rules for $\typety{\Gamma}{\ty}{\ki}$,
  $\typeki{\Gamma}{\ki}{\delta}$, and
  $\typecn{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}$.}
\caption{Type checking rules for terms, continuations, and commands}
\label{fig:typing-rules}
\end{figure}

\begin{figure}
\centering

Binding typing: $\typebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}$ and
$\typebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}$
\begin{gather*}
  \infer
  {\typebind{\Gamma}{\Delta}{\nonrec{\bp}}{\Gamma'}{\Delta'}}
  {\typebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}}
  \qquad
  \infer
  {
    \typebind{\Gamma}{\Delta}{\rec{\vect[n]{\bp_n}}}{\Gamma'}{\Delta'}
  }
  {
    \vect[n]{
      \typebp
      {\Gamma,\Gamma'}{\Delta,\Delta'}
      {\bp_n}
      {\Gamma'_n}{\Delta'_n}
    }
    &
    \Gamma' = \vect[n]{\Gamma'_n}
    &
    \Delta' = \vect[n]{\Delta'_n}
  }
\end{gather*}
\begin{gather*}
  \infer
  {\typebp{\Gamma}{\Delta}{\bindv{\ann x \ty}{\tm}}{x:\ty}{\mt}}
  {\typetm{\Gamma}{\tm}{\ty}}
  \qquad
  \infer
  {
    \typebp
    {\Gamma}{\Delta}
    {\bindk{j}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}}
    {\mt}{j:\ty}
  }
  {\typejmpko{\Gamma}{\Delta}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}{\ty}}
\end{gather*}

Alternative typing: $\typealt{\Gamma}{\Delta}{\analt}{\ty}$
\begin{gather*}
  \infer
  {\typealt{\Gamma}{\Delta}{\alt{\blank}{\cmd}}{\ty}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\typealt{\Gamma}{\Delta}{\alt{\lit}{\cmd}}{\ty}}
  {
    \typetm{\Gamma}{\lit}{\ty}
    &
    \typecmd{\Gamma}{\Delta}{\cmd}
  }
  \\\\
  \infer
  {
    \typealt
    {\Gamma}{\Delta}
    {
      \alt{
        K ~ \vect[m']{\ann{b_{m'}}{\theta(\ki'_{m'})}} \vect[n]{\ann{x_{n}}{\theta(\ty_{n})}}
      }
      {\cmd}
    }
    {T ~ \vect[m]{\ty'_m}}
  }
  {
  \begin{aligned}[b]
    &
    K
    :
    \vect[m]{\forall\ann{a_m}{\ki_m}.} \vect[m']{\forall\ann{b_{m'}}{\ki'_{m'}}.}
      \vect[n]{\ty_{n} \to} T ~ \vect[m]{a_m}
    \in
    \Gamma
    \\
    &
    \typecmd
    {\Gamma, \vect[m']{b_{m'}:\theta(\ki'_{m'})}, \vect[n]{x_{n}:\theta(\ty_{n})}}
    {\Delta}
    {\cmd}
  \end{aligned}
    &
    \theta = \vect[m]{\subst{a_m}{\ty'_m}}
  }
\end{gather*}

Program typing: $\typepgm{\Gamma}{\Delta}{\pgm}$
\begin{gather*}
  \infer
  {\typepgm{\Gamma}{\Delta}{\decl; \pgm}}
  {
    \typedecl{\Gamma}{\decl}{\Gamma'}
    &
    \typepgm{\Gamma,\Gamma'}{\Delta}{\pgm}
  }
  \qquad
  \infer
  {\typepgm{\Gamma}{\Delta}{\cmd}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
\end{gather*}

\emph{Further rules for $\typedecl{\Gamma}{\decl}{\Gamma'}$.}
\caption{Type checking rules for bindings, alternatives, and programs}
\label{fig:typing-rules-binds}
\end{figure}

\begin{figure}
\centering

 Term argument typing: $\typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}$
\begin{gather*}
  \axiom{\typejmptm{\Gamma}{\mt}{\unitjmpty}}
  \qquad
  \infer
  {\typejmptm{\Gamma}{v', \vect[n]{\tm_n}}{\prodjmpty{\ty'}{\ty}}}
  {
    \typetm{\Gamma}{v'}{\ty'}
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}
  }
  \\\\
  \infer
  {\typejmptm{\Gamma}{\ty', \vect[n]{\tm_n}}{\existjmpty{\ann a \ki}{\ty}}}
  {
    \typety{\Gamma}{\ty'}{\ki'}
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty\subst{a}{\ty'}}
  }
\end{gather*}

Parameterized continuation typing:
$\typejmpko{\Gamma}{\Delta}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}{\ty}$
\begin{gather*}
  \infer
  {\typejmpko{\Gamma}{\Delta}{\fnk{}\cmd}{\unitjmpty}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {
    \typejmpko
    {\Gamma}{\Delta}
    {\fnk{\ann{y}{\ty'}~\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\prodjmpty{\ty'}{\ty}}
  }
  {
    \typejmpko
    {\Gamma, y:\ty'}{\Delta}
    {\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\ty}
  }
  \\\\
  \infer
  {
    \typejmpko
    {\Gamma}{\Delta}
    {\fnk{\ann{a}{\ki}~\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\existjmpty{\ann{a}{\ki}}\ty}
  }
  {
    \typejmpko
    {\Gamma, a:\ki}{\Delta}
    {\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\ty}
  }
\end{gather*}

\caption{Type checking rules for parameterized continuations and their arguments}
\label{fig:typing-rules-jumps}
\end{figure}

The typing rules for Dual System FC are given in Figures~\ref{fig:typing-rules},
\ref{fig:typing-rules-binds}, and \ref{fig:typing-rules-jumps}.  The type of a
term classifies the results that it might produce, and the type of a
continuation classifies the results that it expects to consume.  Commands do not
have a type; they are just $\Ok$ to run.  The eventual result of a command is
returned through the unique return continuation available in its environment.
Likewise, a program is a consistent block of code that is capable of running,
meaning that a program is a command that runs with respect to some top-level
declarations that introduce type information (data types, type synonyms, and
axioms).  The normal way to type-check a top-level program is as
$\typepgm{\Gamma_0}{\ret\tp:\ty}{\pgm}$, where $\Gamma_0$ specifies any
primitive types and values provided by the run-time environment, and
$\ret\tp:\ty$ is the single, top-level exit path out of the program that expects
a $\ty$ result.

Compared with System FC, more of the typing rules enjoy the
\emph{sub-formula property}, meaning that the types appearing in a premise above
the line of a rule appear somewhere below the line.  This is a natural
consequence of the sequent calculus as a logic, and was one of the primary
motivations for its original development.  The expected rules violating the
sub-formula property are the various \emph{cut} rules for commands that cancel
out arbitrary types, where the $\Let$-form of command allows us to perform
multiple cuts simultaneously.  The other interesting violators of note are:
\begin{itemize}
\item The rule for pushing a frame onto the stack,
  $\frm' \then \vect[n]{\frm_n}$, which is effectively another form of cut
  between the output type of $\frm'$ and the input type of $\vect[n]{\frm_n}$.

\item The rule for the cast frame $\castby{\cn}$, which eliminates the equality
  type $\eqty{\ty}{\ty'}$ into the input/ispell typing judgement of a frame.
  This is a less severe infraction, since the two are isomorphic, no essential
  information is gained or lost.

% \item The rule for a polymorphic call-stack,
%   $\app{\ty_1}{\ko} : \forall\ann{a}{\ki}.\ty_2$, which substitutes the
%   specified type $\ty_1$ in for the variable $a$ in $\ty_2$ to get the type for
%   the continuation.  This rule does not have the sub-formula property since
%   $\ty_2\subst{a}{\ty_1}$ is a new type generated by the substitution.

%   Since universal quantification is dual to existential quantification, the
%   polymorphic call-stack is dual to the existential pair
%   $(\tau_1,\tm) : \exists\ann{a}{\ki}.\tau_2$, and shares the same properties.
%   In particular, $\app{\tau_1}{\ko}$ does not have a \emph{unique} type.  For
%   example, given the continuation variable $r$ of type $Bool$, then the
%   polymorphic call-stack $\app{Int}{\app{1}{\app{2}{\ret q}}}$ can be given the
%   types $\forall \ann{a}{\star}. a \to a \to Bool$,
%   $\forall \ann{a}{\star}. Int \to a \to Bool$, and so on.  So following the
%   bottom-up preference of a sequent calculus presentation, it is easy to type
%   check a polymorphic call-stack if we already know the type of function it
%   expects, but in general it is hard to guess its type.

% \item The rules for pattern matching on data types almost suffers from the same
%   issue as for polymorphic call-stacks, due to substitution of the choice for
%   polymorphic type variables.  However, the type annotations on variables bound
%   by pattern matching already specify the specialized types, so the issue is
%   avoided.

%   Also note that the problem with existential pairs (or in general, existential
%   data structures) mentioned in the previous point is avoided on the term side.
%   This is because we do not represent data structures directly, as a
%   fully-applied constructor like $(\ty, \tm)$, but rather we represent them
%   indirectly as a constructor evaluated with a polymorphic call-stack,
%   $\cut{(,)}{\app{\ty}{\app{\tm}{\ret q}}}$.  That means that all the troubles
%   with checking user-defined existential data structures are contained solely in
%   polymorphic call stacks.

% \item The rule for coercion continuations, $\koerce{\cn}{\ko} : \ty_1$, in which
%   the type of the continuation $\ko$ is hidden by the coercion.  Interestingly,
%   the typing rules for casting resembles a special sort of function application,
%   both with terms in System FC and continuations in Dual System FC.
\end{itemize}
Observe that by factoring out frames from the final continuation, we actually
improve the sub-formula property for polymorphic instantiation.  The polymorphic
call stack $\app{\ty}{\ko}$ normally lacks the sub-formula property since $\ty$
is substituted into the type of input that $\ko$ expects to receive.  As a
single frame, however, $\inst{\ty}$ specifies both the polymorphic schema for
the input it receives as well as the instantiated output it produces below the
line of inference.

Notice that the unique types of terms and final continuations (either the return
continuation $\ret q$ or case continuation
$\caseas{\ann x \ty}{\vect[n]{\analt_n}}$) are easy to infer.  However, frames
do \emph{not} have a unique type.  For example, the application frame
$\appto{1}$ can take an input of type $Int \to \ty'$ and produce an output of
type $\ty'$, for any choice of $\ty'$.  Even worse, the frame $\inst{\ty}$ for
polymorphic instantiation connects any universally quantified type
$\forall a:\ki.\ty'$ to the instantiated one $\ty'\subst{a}{\ty}$.  Thus, while
terms and final continuations support a simple type inference procedure, frames
do not.  Instead, we just check that a frame (or stack of frames) fits a chosen
type.  Additionally, the type of the term arguments, $\vect{\tm}$, is easy to
check but hard to infer, whereas the type of the parameterized continuation
$\fnk{\vect{x:\ty}}\cmd$ is easy to infer.  This flow of type-inferring versus
type-checking is captured in the structure of a command and gives us a type
checking algorithm:
\begin{itemize}
\item If the command is a $\Let$-binding, gather the types for the bound local
  variables and confirm that they are bound to well-typed terms and
  continuations.
\item If the command evaluates a term in the eye of a continuation,
  $\braket{\tm | \vect[n]{\frm_n} | \ko}$, then read the type off of $\tm$ and
  $\ko$, thereby confirming that they are well-typed, and check that
  $\vect[n]{\frm_n}$ connects these two types.
\item If the command is a jump, $\jump{\vect{\tm}}{j}$, then read the type off
  of $j$, and check that $\vect{\tm}$ has the same type.
\end{itemize}

% Due to the difficulty of inferring the type of a polymorphic call-stack, the
% type checking algorithm for Sequent Core reveals some asymmetrical bias of type
% information.  As designed, reading the type off of a Sequent Core term is always
% straightforward.  However, continuations may have several different types (due
% to the polymorphic form of call stacks).  Instead, it is straightforward to
% check if a continuation has a particular type.  Contrarily, the type of the term
% arguments, $\Jump\vect{\tm}$, is easy to check but hard to infer, whereas the
% type of the parameterized continuation $\fnk{\vect{x:\ty}}\cmd$ is easy to
% infer.  This flow of type-inferring versus type-checking is captured in the
% structure of a command and gives us a type checking algorithm:
% \begin{itemize}
% \item If the command is a $\Let$-binding, gather the types for the bound local
%   variables and confirm that they are bound to well-typed terms and
%   continuations.
% \item If the command evaluates a term in the eye of a continuation,
%   $\cut{\tm}{\ko}$, then read the type off of $\tm$, thereby confirming that it
%   is well-typed, and check that $\ko$ has the same type.
% \item If the command is a jump, $\jump{\vect{\tm}}{j}$, then read the type off
%   of $j$, and check that $\Jump\vect{\tm}$ has the same type.
% \end{itemize}
% Determining the explicit flow of type information (from term to continuation or
% continuation to term, depending on the form of the command) likely has something
% to do with the traditional form of bi-directional type checking from the
% literature.  Intuitively, the problem with inferring the type of polymorphic
% call stacks is identical to the problem with inferring the general existential
% tuples in the sequent calculus.  Thus, solving the problem with forall gives us
% a solution for exists by duality in the sequent calculus.  Symmetry saves the
% day!

\section{Semantics}
\label{sec:semantics}

Here we illustrate the semantics of Sequent Core programs in two forms:
\begin{itemize}
\item An equational theory of program transformations that a compiler might
  perform, in any order, to simplify a Sequent Core program.
\item A small-step, operational semantics that suggests how an interpreter might
  evaluate a Sequent Core program.
\end{itemize}
For simplicity, the presented semantics Sequent Core only accounts for
call-by-name evaluation and non-recursive bindings.  A more thorough semantics
would combine both call-by-name evaluation (for lifted types) and call-by-value
evaluation (which is necessary for unlifted types), along with recursive
$\Let$-bindings.

\subsection{Equational Theory}

First, we have the sequent equivalent of the usual $\beta$ and $\eta$ axioms for
functions:
\begin{align*}
  \braket{\fn{\ann{x}{\ty}} \tm | \appto{\tm'} \then \vect[n]{\frm_n} | \ko}
  &=
  \Let \nonrec{\bindv{\ann{x}{\ty}}{\tm'}} \In \braket{\tm | \vect[n]{\frm_n} | \ko}
  &
  \Where
  x &\notin FV(\ko)
  \\
  \fn{\ann{x}{\ty}} \tm \appto{x}
  &=
  \tm
  &
  \Where
  x &\notin FV(\tm)
  \\
  &&
  \tm &\neq \bot : \ty \to \ty'
\end{align*}
For the $\beta$ axiom, we simplify a $\lambda$-abstraction that is evaluated
with respect to a calling continuation by $\Let$-binding the argument and
evaluating the body in the return return continuation.  For the $\eta$ axiom, we
recognize that the functional abstraction
$\fn{x}f\appto{x}=\fn{x}\comp{r}\braket{f | \appto{x} | \ret r}$ immediately
delegates to $f$, so the two are equivalent.  Note that, due to the presence of
unconstrained, polymorphic strictness (for example, \texttt{seq}), the $\eta$
axiom can only be applied to terms of a function type that do not evaluate to
bottom.  In other words, the $\eta$ axiom only applies to terms that are already
equal to a $\lambda$-abstraction without the use of $\eta$.

Computation abstractions (that is, $\mu$-abstractions), also follow a form of
$\beta$ and $\eta$ axioms:
\begin{align*}
  \cut{\comp{r}\cmd}{\ko}
  &=
  \cmd\subst{\ret r}{\ko}
  &
  \comp{r}\cut{\tm}{\ret r}
  &=
  \tm
  % &
  % \Where
  % r \notin FV(\tm)
\end{align*}
The $\beta$ axiom for a $\mu$-abstraction substitutes its entire continuation
for $\ret r$.  To avoid unnecessary code duplication, we may only want to apply
the $\beta$ axiom for $\mu$ when $k$ is small (and we can force $k$ to be small
by introducing auxiliary bindings naming arguments and join points).  The $\eta$
axiom for a $\mu$-abstraction is more well-behaved than the one for functions:
it applies to \emph{any} term that does not reference the bound continuation
variable.

Case continuations have their own form of $\beta$ and $\eta$ axioms.  The
$\beta$ axiom for a case continuation selects the appropriate branch based on
the structure of input, in the case that it is given a literal or a constructed
data structure.
\begin{align*}
  \cut{\lit}{\caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd};\dots}}
  &=
  \cmd\subst{x}{\lit}
\end{align*}
\begin{align*}
  &
  \braket
  { K
  | 
    \vect[n]{\appto{\tm_n}}
  | \caseas{\ann{x}{\ty}}{
      \dots; \alt{K ~ \vect[n]{\ann{y_n}{\sty_n}}}{\cmd}; \dots
    } }
  \\
  &=
  \vect[n]{
  \Let
    \nonrec{\bindv{\ann{y_n}{\sty_n}}{\tm_n}}
  \In
  }
  \Let \nonrec{
    \bindv{\ann{x}{\ty}}{K \vect[n]{\appto{y_n}}}
  } \In
    \cmd
\end{align*}
The default alternative for a case continuation is selected when no other
alternative matches, and only applies when the continuation is given a weak-head
normal form (WHNF) as input.
\begin{align*}
  \braket
  { \tm
  | \vect[n]{\frm_n}
  | \caseas{\ann{x}{\ty}}{\dots;\alt{\blank}{\cmd}} }
  &=
  \Let
    \nonrec{\bindv{\ann{x}{\ty}}{\tm ~ \vect[n]{\frm_n}}}
  \In
    \cmd
  \\
  \Where
  \tm ~ \vect[n]{\frm_n}
  &\in
  WHNF
\end{align*}
Note that the list of arguments, $\vect[n]{\frm_n}$, can only be non-empty when
the head term $\tm$ is a constructor: $K ~ \vect[n]{\frm_n}$ is a WHNF.  The
other possibility is that $\vect[n]{\frm_n}$ is empty and $\tm$ is a literal or
a $\lambda$-abstraction.  We also have some extensional properties about case
continuations.  First, we may replace the name for the input with the pattern in
any alternative, or vice versa:
\begin{align*}
  \caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd};\dots}
  &=
  \caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd\subst{x}{\lit}};\dots}
\end{align*}
\begin{align*}
  &
  \caseas{\ann{x}{\ty}}{\dots;\alt{K ~ \vect[n]{\ann{y_n}{\sty_n}}}{\cmd};\dots}
  \\
  &=
  \caseas{\ann{x}{\ty}}{
    \dots;
    \alt
    {K ~ \vect[n]{\ann{y_n}{\sty_n}}}
    {\cmd\subst{x}{K ~ \vect[n]{\appto{y_n}}}};
    \dots
    }
\end{align*}
Second, we have a form of $\eta$ axiom for case continuations which eliminates
a redundant case analysis wrapped around another continuation.
\begin{align*}
  \caseas{\ann{x}{\ty}}{\vect[n]{\alt{\pat_n}{\cut{x}{\ko}}}}
  &=
  \ko
  &
  \Where{}
  \{x\} \cup BV(\vect[n]{\pat_n}) &\notin FV(\ko)
  ,
  \ko : \ty
\end{align*}
These extensionality laws are dual to the $\eta$ laws for $\lambda$- and
$\mu$-abstractions.  For instance, for pairs, we have the dual equality to
functional $\eta$:
\begin{align*}
  \Case\Of \alt{(\ann{x}{\ty_1},\ann{y}{\ty_2})}{\cut{(x,y)}{\ko}}
  &=
  \ko
  &
  \Where
  x,y &\notin FV(\ko)
  ,
  \ko : (\ty_1, \ty_2)
\end{align*}
Furthermore, for a case continuation with only a default alternative, we have
the dual equality to $\mu$ $\eta$:
\begin{align*}
  \Case\Of \alt{x}{\cut{x}{\ko}}
  &=
  \ko
  &
  \Where
  x &\notin FV(\ko)
\end{align*}

The $\Let$ form of commands substitute their bindings into the underlying
command.  First, we interpret each type of binding pair as a substitution:
\begin{align*}
  \subp{\bindv{x}{\tm}} &= \subst{x}{\tm}
  \\
  \subp{\bindk{j}{\fnk{\vect[n]{x_n}}\cmd}}
  &=
  \subst{\vect[n]{\Let \nonrec{\bindv{x_n}{\tm_n}} \In} \cmd}{\jump{\vect[n]{\tm_n}}{j}}
\end{align*}
The substitution of a value binding is the normal capture-avoiding substitution
of terms.  The substitution of a continuation binding  Then a $\Let$-binding command substitutes its binding pair:
\begin{align*}
  \Let \nonrec{\bp} \In \cmd
  &=
  \cmd\subp{\bp}
\end{align*}

We now consider axioms for pushing casts around the program.  A sequence of
casts can be combined into a single cast of the composed coercion:
\begin{align*}
  \castby{\cn} \then \castby{\cn'}
  &=
  \castby{(\compcn{\cn}{\cn'})}
\end{align*}
Casting a coercion itself can be reduced to a modified coercion that pipes the
type conversions together:
\begin{align*}
  \braket{\cn | \castby{\cn'} \then \vect[n]{\frm_n} | \ko }
  &=
  \braket{\compcn{\cn'_0}{\compcn{\cn}{\cn'_1}} | \vect[n]{\frm_n} | \ko}
  &
  \Where
  \cn &: \eqty{\ty_0}{\ty_1}
  \\ &&
  \cn' &: \eqty{(\eqty{\ty_0}{\ty_1})}{(\eqty{\ty'_0}{\ty'_1})}
  \\ &&
  \cn'_0 &: \eqty{\ty'_0}{\ty_0} = \sym(\nth_0 \cn')
  \\ &&
  \cn'_1 &: \eqty{\ty_1}{\ty'_1} = \nth_1 \cn'
\end{align*}
In general, we will always push casts down structures.  For coercions between
function type, this means pushing the cast down the call-stack:
\begin{align*}
  \castby{\cn} \then \appto{\tm}
  &=
  \appto{(\tm \castby{\cn_0})} \then \castby{\cn_1}
  \\
  \Where
  \cn &: \eqty{(\ty_0 \to \ty_1)}{(\ty_0' \to \ty_1')}
  \\
  % \tm &\notin Type \cup Coercion
  \tm &\notin Type
  \\
  \cn_0 &: \eqty{\ty'_0}{\ty_0} = \sym (\nth_0 \cn)
  \\
  \cn_1 &: \eqty{\ty_1}{\ty'_1} = \nth_1 \cn
\end{align*}
Similarly, casts down polymorphic call-stacks for coercions between universally
quantified types:
\begin{align*}
  \castby{\cn} \then \appto{\ty}
  &=
  \appto{\ty} \then \castby{\cn'}
  \\
  \Where
  \cn &: \eqty{\forall a:\ki. \ty'_1}{\forall a:\ki. \ty'_2}
  \\
  \cn' &: \eqty{\ty'_1\subst{a}{\ty}}{\ty'_2\subst{a}{\ty}} = \instancn{\cn}{\ty}
\end{align*}
Casting a literal is a no-op, so long as the coercion represents the reflexive
equality of the literal type with itself:
\begin{align*}
  \cut{\lit}{\koerce{\cn}{\ko}}
  &=
  \cut{\lit}{\ko}
  &
  \Where
  \cn &: \eqty{\ty}{\ty}
  ,
  \lit : \ty
\end{align*}
The most complicated push axiom is for data types.  Here, we push a cast on a
constructed data structure into its sub-terms:
\begin{align*}
  \braket
  { K
  | \vect[m]{\inst{\ty_{m}}}
    \then \vect[m']{\inst{\sty_{m'}}}
    \then \vect[n]{\appto{\tm_n}}
    \then \castby{\cn}
  | \ko }
  &=
  \braket
  { K
  | \vect[m]{\inst{\ty'_{m}}}
    \then \vect[m']{\inst{\sty_{m'}}}
    \then \vect[n]{\appto{(\tm_n \castby{\cn'_n})}}
  | \ko }
  \\
  \Where
  \cn &: \eqty{T ~ \vect[m]{\ty_m}}{T ~ \vect[m]{\ty'_m}}
  \\
  K
  &:
  \ty_K
  =
  \vect[m]{\forall a_{m}:\ki_{m}.}
  \vect[m']{\forall b_{m'}:\ki'_{'m}.}
  \vect[n]{\sty'_n \to} T ~ \vect[m]{a_{m}}
  \\
  \cn'_i
  &:
  \eqty
  {(\sty'_i\vect[m]{\subst{a_{m}}{\ty_m}}\vect[m']{\subst{b_{m'}}{\sty_{m'}}})}
  {(\sty'_i\vect[m]{\subst{a_{m}}{\ty'_m}}\vect[m']{\subst{b_{m'}}{\sty_{m'}}})}
  \\
  \cn'_i &= \ntharg_i(\idcn{\ty_K} ~ \vect[m]{\nth_{m} \cn} ~ \vect[m']{\idcn{\sty_{m'}}})
\end{align*}
Part of the complication is the different behavior of universally and
existentially quantified types in the structure.  The universally quantified
types, $\vect[m]{\ty_{m}}$, are replaced the type of the coercion, representing
a change in the externally visible type of the structure itself.  The
existentially quantified types, $\vect[m']{\sty_{m'}}$, don't change at all, as
they are a hidden internal part of the structure.  The value sub-components of
the structure, $\vect[n]{\tm_n}$, are get cast by the new coercions
$\vect[n]{\cn'_n}$.  These new coercions are formed by selecting the matching
argument type from the type of the constructor, and replacing the original
universally quantified types with the coerced ones.

\subsection{Operational Semantics}

Since we do not consider recursive bindings, the reduction steps in operational
semantics of Sequent Core always apply the to the top of a command.  The purely
structural reductions, which substitute $\Let$- and $\mu$-bindings, are:
\begin{align*}
  \Let \nonrec{\bp} \In \cmd
  &\sred
  \cmd\subp{\bp}
  \\
  \braket{\comp{r}\cmd | \vect[n]{\frm_n} | \ko}
  &\sred
  \cmd
  \subst
    {\braket{\tm | \vect[m]{\frm'_m} | \ret r}}
    {\braket{\tm | \vect[m]{\frm'_m} ; \vect[n]{\frm_n} | \ko}}
  % \\
  % \braket{\tm | \vect[n]{\frm_n} | \vect[m]{\frm_m} \then \ko}
  % &\sred
  % \braket{\tm | \vect[n]{\frm_n} \then \vect[m]{\frm_m} | \ko}
\end{align*}

\begin{align*}
  \braket{ \fn x \tm | \appto{\tm'} \then \vect[n]{\frm_n} | \ko}
  &\sred
  \braket{\tm\subst{x}{\tm'} | \vect[n]{\frm_n} | \ko}
  % \\
  % \braket{ \fn a \tm | \inst{\ty} \then \vect[n]{\frm_n} | \ko }
  % &\sred
  % \braket{\tm\subst{a}{\ty} | \vect[n]{\frm_n} | \ko}
\end{align*}

Recall that we use the syntactic sugar that
$\tm\then\vect[n]{\frm_n}=\comp{r}\braket{\tm|\vect[n]{\frm_n}|\ret{r}}$. The
rules for reducing case continuations are:
\begin{align*}
  \cut{\lit}{\caseas{x}{\dots;\alt{\lit}{\cmd};\dots}}
  &\sred
  \cmd\subst{x}{\lit}
  \\
  \braket
  { K
  | \vect[m]{\inst{\ty_{m}}} \then \vect[n]{\appto{\tm_{n}}}
  | \caseas{x}{\dots;\alt{K ~ \vect[n]{y_n}}{\cmd};\dots}
  }
  &\sred
  \cmd
  \subst{x}{(K \then \vect[m]{\appto{\ty_{m}}} \then \vect[n]{\appto{\tm_{n}}})}
  \vect[n]{\subst{y_n}{\tm_n}}
  \\
  \braket
  { \tm
  | \vect[n]{\frm_n}
  | \caseas{x}{\dots;\alt{\blank}{\cmd}} }
  &\sred
  \cmd\subst{x}{\tm \then \vect[n]{\frm_n}}
  \\
  \Where \tm \then \vect[n]{\frm_n} &\in WHNF
\end{align*}
Note that the set of weak head normal forms ($WHNF$) includes literals,
$\lambda$-abstractions, coercions, and constructed terms of the form
$K \then \vect[n]{\frm_n}$.

Finally, we have the reductions which push casts out of the way of $\beta$
reductions at the last moment.  For applications, we have:
\begin{align*}
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \castby{\cn}
    \then \castby{\cn'}
    \then \vect[n]{\frm_n}
  | \ko }
  &\sred
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \castby{(\compcn{\cn}{\cn'})}
    \then \vect[n]{\frm_n}
  | \ko }
  \\
  \Where
  \tm'; \vect[m]{\appto{\tm_m}} &\in WHNF
  \\
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \castby{\cn}
    \then \appto{\tm'}
    \then \vect[n]{\frm_n}
  | \ko }
  &\sred
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \appto{(\tm' \then \castby{\sym(\nth_0 \cn)})}
    \then \castby{(\nth_1 \cn)}
    \then \vect[n]{\frm_n}
  | \ko }
  \\
  \Where
  \cn &: \eqty{(\ty_1 \to \ty_2)}{(\ty'_1 \to \ty'_2)}
  \\
  \tm'; \vect[m]{\appto{\tm_m}} &\in WHNF
  \\
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \castby{\cn}
    \then \inst{\ty}
    \then \vect[n]{\frm_n}
  | \ko }
  &\sred
  \braket
  { \tm'
  | \vect[m]{\appto{\tm_m}}
    \then \inst{\ty}
    \then \castby{(\instancn{\cn}{\ty})}
    \then \vect[n]{\frm_n}
  | \ko }
  \\
  \Where
  \cn &: \eqty{(\forall a:\ki.\ty')}{(\forall a:\ki.\ty'')}
  \\
  \tm'; \vect[m]{\appto{\tm_m}} &\in WHNF
\end{align*}
For a matching literal case, we have:
\begin{align*}
  \braket{\lit | \castby{\cn} | \caseas{x}{\dots;\alt{\lit}{\cmd};\dots} }
  &\sred  
  \cut{\lit}{\caseas{x}{\dots;\alt{\lit}{\cmd};\dots}}
\end{align*}
For a matching constructed term, we have:
\begin{align*}
  &
  \Braket
  { K
  | \vect[m]{\inst{\ty_{m}}}
    \then \vect[m']{\inst{\sty_{m'}}}
    \then \vect[n]{\appto{\tm_{n}}}
    \then \castby{\cn}
  | \caseas{x}{\dots;\alt{K ~ \vect[m']{b_{m'}} \vect[n]{y_{n}}}{\cmd};\dots} }
  \\
  &\sred
  \Braket
  { K
  | \vect[m]{\inst{\ty'_{m}}}
    \then \vect[m']{\inst{\sty_{m'}}}
    \then \vect[n]{\appto{(\tm_{n} \then \castby{\cn'_n})}}
  | \caseas{x}{\dots;\alt{K ~ \vect[m']{b_{m'}} \vect[n]{y_{n}}}{\cmd};\dots} }
  \\
  &\qquad
  \Where
  \cn : \eqty{T ~ \vect[m]{\ty_{m}}}{T ~ \vect[m]{\ty'_{m}}}
  \\
  &\qquad\phantom{\Where}
  K 
  :
  \vect[m]{\forall\ann{a_{m}}{\ki_{m}}.}
  \vect[m']{\forall\ann{b_{m'}}{\ki'_{m'}}.}
  \vect[n]{\sty'_{n} \to} T ~ \vect[m]{a_{m}}
  \\
  &\qquad\phantom{\Where}
  \cn'_i
  =
  \sty'_i\vect[m]{\subst{a_{m}}{\nth_m \cn}}\vect[m']{\subst{b_{m'}}{\idcn{\sty_{m'}}}}
\end{align*}

% And finally, when we have two casts in a row, we merge them into a single cast
% of the composed coercion:
% \begin{align*}
%   \braket
%   { \V
%   | \castby{\cn} \then \castby{\cn'} \then \vect[n]{\frm_n}
%   | \ko}
%   &\sred
%   \braket
%   { \V
%   | \castby{(\compcn{\cn}{\cn'})} \then \vect[n]{\frm_n}
%   | \ko }
% \end{align*}

\section{Translation}
\label{sec:translation}

An important aspect of Sequent Core is that it can be translated both to and
from Core, which has some benefits:
\begin{itemize}
\item We are sure that Sequent Core are at least as expressive as Core, so that
  it can represent every Core program and type.
\item We are sure that Sequent Core is not \emph{more} expressive than Core,
  which is a prevailing concern since the sequent calculus (like
  continuation-passing style) is a natural setting for first-class control.  We
  don't want to introduce unusual control flow that can't be represented at
  least somewhat directly in Core.
\item The compiler can process a program represented in Core for a bit, then
  represented in Sequent Core, then back in Core again.  This capability is what
  makes our use of the plugin architecture possible, so that we can add Sequent
  Core to GHC without modifying GHC itself!
\end{itemize}

\subsection{From Core to Sequent Core}

Let's start with the simplest translation of Core into Sequent Core: a
compositional translation of Core expressions into Sequent Core terms,
$Seq\trans{\expr}$.  Translating apparent%
\footnote{We say ``apparent'' because the structures of a data type are not
  expressed directly in Core, but are written as a chain of function
  applications with a constructor (a special sort of variable identifier) at the
  head.  Since $K~x~y~z$ ``looks like'' a function application at first glance,
  it is apparently not a value even though in actuality it is.}
 %
values from Core is not hard:
\begin{align*}
  Seq\trans{x} &= x
  \\
  Seq\trans{\lit} &= \lit
  \\
  Seq\trans{\fn{\ann{x}{\ty}} \expr}
  &=
  \fn{\ann{x}{\ty}} Seq\trans{\expr}
  \\
  Seq\trans{\cn} &= \cn
  \\
  Seq\trans{\ty} &= \ty
\end{align*}
Rather, most of the work of translating Core into Sequent Core is in handling
the apparent computation.  In the compositional translation, apparent
computation expressions all correspond to computation $\mu$-abstractions.
\begin{align*}
  Seq\trans{\expr_1 ~ \expr_2}
  &=
  \comp{\ann r \ty}
    \braket
    { Seq\trans{\expr_1}
    | \appto{Seq\trans{\expr_2}}
    | \ret r }
  % \\
  % &
  % \Where{}
  %   (\expr_1 ~ \expr_2) : \ty
  \\
  Seq\trans{\Let \abind \In \expr}
  &=
  \comp{\ann r \ty}\Let Seq\trans{\abind} \In \cut{\expr}{\ret r}
  % \\
  % &
  % \Where{}
  %   (\Let \abind \In \expr) : \ty
  \\
  Seq\trans{\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}}
  &=
  \comp{\ann{r}{\ty}}
    \cut
    {Seq\trans{\expr}}
    {\caseas{\ann{x}{\ty}}{\vect[n]{Seq\trans{\analt_n}}}}
  % \\
  % &
  % \Where{}
  %   (\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}) : \ty
  \\
  Seq\trans{\coerce{\expr}{\cn}}
  &=
  \comp{\ann{r}{\ty}}\braket{Seq\trans{\expr}|\castby{\cn}|\ret{r}}
  % \\
  % &
  % \Where{}
  %   (\coerce{\expr}{\cn}) : \ty
\end{align*}
% Because the continuation variable $r$ introduced by the computation abstractions
% is annotated with its type, we need to infer the type of all apparent
% computations to determine this annotation.
We also need to translate the binding of a $\Let$ expression and alternatives of
a case expression:
\begin{align*}
  Seq\trans{\nonrec{\ann x \ty = \expr}}
  &=
  \nonrec{\bindv{\ann x \ty}{Seq\trans{\expr}}}
  \\
  Seq\trans{\rec{\vect[n]{\ann{x_n}{\ty_n} = \expr_n}}}
  &=
  \rec{\vect[n]{\bindv{\ann{x_n}{\ty_n}}{Seq\trans{\expr_n}}}}
\end{align*}
\begin{align*}
  Seq\trans{\blank \to \expr}%_{r}
  &=
  \blank \to \cut{Seq\trans{\expr}}{\ret r}
  \\
  Seq\trans{
    K ~ \vect[n]{\ann{x_n}{\ty_n}} \to \expr
  }%_{r}
  &=
  K ~ \vect[n]{\ann{x_n}{\ty_n}}
  \to
  \cut{Seq\trans{\expr}}{\ret r}
  \\
  Seq\trans{\lit \to \expr}%_{r}
  &=
  \lit \to \cut{Seq\trans{\expr}}{\ret r}
\end{align*}
% Note that because case alternatives in Sequent Core point to self-contained
% commands, we explicitly spell out the common continuation that every alternative
% ``returns'' to, which was introduced as the return continuation for the case
% expression itself.
Finally, we can translate a whole program from Core to Sequent Core which gives
its final result on $\ret r$:
\begin{align*}
  Seq\trans{\vect[n]{\decl_n}; e}%_{r}
  &=
  \vect[n]{\decl_n}; \cut{Seq\trans{e}}{\ret r}
\end{align*}

While the compositional translation is simple, it creates unnecessarily large
terms due to the fact that every apparent computation gets its own
$\mu$-abstraction.  We can then take the observation on the difference between
apparent values and apparent computations in Core to write a better, more
compacting translation into Sequent Core.  This more compacting translation is
closer to the implemented one, but still quite simplified.%
\footnote{The implemented translation handles the renaming necessary to avoid
  static variable capture and also attempts to translate functions which
  represent continuations as continuations (i.e., it performs some
  \emph{re-contification}).}
 %
For apparent values (variables, literals, lambda abstractions, coercions, and
types), the translation is much the same:
\begin{align*}
  Seq\trans{x} &= x
  \\
  Seq\trans{\lit} &= \lit
  \\
  Seq\trans{\fn{\ann{x}{\ty}} \expr}
  &=
  \fn{\ann{x}{\ty}} Seq\trans{\expr}
  \\
  Seq\trans{\cn} &= \cn
  \\
  Seq\trans{\ty} &= \ty
  \\
  Seq\trans{\expr}
  &= \comp{r:\ty} (Seq\trans{\expr} \mt ~ \ret{r})
  &
  \Where
  \expr & \text{ is an apparent computation}
\end{align*}
The main difference is when we find an apparent computation (an application, let
expression, case expression, or cast), identified by the last clause above.  In
this case, we introduce \emph{one} $\mu$-abstraction, and then begin to collect
the outer-most frames of the computation:
\begin{align*}
  Seq\trans{\expr_1 ~ \expr_2} \vect[n]{\frm_n}  ~ \ko
  &=
  Seq\trans{\expr_1} (\appto{Seq\trans{\expr_2}};\vect[n]{\frm_n}) ~ \ko
  \\[1ex]
  Seq\trans{\Let \abind \In \expr} \vect[n]{\frm_n} ~ \ko
  &=
  \Let Seq\trans{\abind} \In Seq\trans{\expr} \vect[n]{\frm_n} ~ \ko
  \\
  \Where
    BV(\abind) &\notin FV(\ko)
  \\[1ex]
  Seq\trans{
  \begin{aligned}
   &\Case \expr \As \ann{x}{\ty} \Of
   \\
   &\qquad
   \vect[m]{\analt_m} 
  \end{aligned}
  }
  \vect[n]{\frm_n} ~ \ko
  &=
  \Braket
    { \comp{\ann{r}{\ty'}} (Seq\trans{\expr} \mt ~ \ko')
    | \vect[n]{\frm_n}
    | \ko }
  \\
  \Where{}
    \ko' &=
       \begin{aligned}[t]
         &
         \Case \As \ann{x}{\ty} \Of
         % \\
         % &\qquad
         \vect[m]{Core\trans{\analt_m} \mt ~ (\ret r)}
       \end{aligned}
  % \\ & \phantom{\Where{}}
    % (\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}) : \ty'
  \\[1ex]
  Seq\trans{\coerce{\expr}{\cn}} \vect[n]{\frm_n} ~ \ko
  &=
  Seq\trans{\expr} (\castby{\cn} \then \vect[n]{\frm_n}) ~ \ko
  \\[1ex]
  Seq\trans{\expr} \vect[n]{\frm_n} \ko
  &=
  \braket{Seq\trans{\expr} | \vect[n]{\frm_n} | \ko}
  \qquad
  \Where
    \expr \text{ is an apparent value }
\end{align*}
Some care must still be taken when translating $\Case$ expressions, since we want
to avoid unnecessary duplication of large continuations inside the branches of
alternatives.  For now, we just check if the continuation to a $\Case$ is small
enough to duplicate, and if not, bind it with a computation abstraction.  When
we reach an apparent value again, in the last clause above, we write down the
entire continuation and list of bindings found during translation.

The more compacting translation of bindings, alternatives, and whole programs
are effectively the same as before, except that for a binding we begin expecting
the bound expression to be value-like, and for an alternative and whole program
we begin expecting the resulting expression to be computation-like:
\begin{align*}
  Seq\trans{\nonrec{\ann x \ty = \expr}}
  &=
  \nonrec{\bindv{\ann x \ty}{Seq\trans{\expr}}}
  \\
  Seq\trans{\rec{\vect[n]{\ann{x_n}{\ty_n} = \expr_n}}}
  &=
  \rec{\vect[n]{\bindv{\ann{x_n}{\ty_n}}{Seq\trans{\expr_n}}}}
\end{align*}
\begin{align*}
  Seq\trans{\blank \to \expr} \vect[n]{\frm_n} ~ \ko
  &=
  \blank \to Seq\trans{\expr} \vect[n]{\frm_n} ~ \ko
  \\
  Seq\trans{K ~ \vect[n]{\ann{x_n}{\ty_n}} \to \expr} \vect[n]{\frm_n} ~ \ko
  &=
  K ~ \vect[m]{\ann{x_m}{\ty_m}}
  \to
  Seq\trans{\expr} \vect[n]{\frm_n} ~ \ko
  \\
  Seq\trans{\lit \to \expr} \vect[n]{\frm_n} ~ \ko
  &=
  \lit \to Seq\trans{\expr} \vect[n]{\frm_n} ~ \ko
\end{align*}
\begin{align*}
  Seq\trans{\vect[n]{\decl_n}; \expr}%_{r}
  &=
  \vect[n]{\decl_n}; Seq\trans{\expr} \mt ~ (\ret r)
\end{align*}

\subsection{From Sequent Core to Core}

Translating Sequent Core terms back into Core expressions is also fairly
straightforward, given by $Core\trans{\tm}$:
\begin{align*}
  Core\trans{x} &= x
  \\
  Core\trans{\comp{\ann r \ty} \cmd} &= Core\trans{\cmd}_{r}
  \\
  Core\trans{\fn{\ann x \ty}\tm} &= \fn{\ann x \ty}Core\trans{\tm}
  \\
  Core\trans{\lit} &= \lit
  \\
  Core\trans{\cn} &= \cn
  \\
  Core\trans{\ty} &= \ty
\end{align*}
All but one Sequent Core term, namely the computation abstraction, corresponds
directly to a Core value.  To translate a computational term to a Core
expression, we need to translate the underlying command and read off the result
returned (to $\ret r$), as given by $Core\trans{\cmd}$:
\begin{align*}
  Core\trans{\Let \abind \In \cmd}
  &=
  \Let Core\trans{\abind} \In Core\trans{\cmd}
  \\
  Core\trans{\braket{\tm | \vect[n]{\frm_n} | \ko}}
  &=
  Core\trans{\ko}[\vect[n]{Core\trans{\frm_n}}[Core\trans{\tm}]]
  \\
  Core\trans{\jump{\vect[n]{\tm_n}}{j}}
  &=
  j ~ \vect[n]{Core\trans{\tm_n}}
\end{align*}
A $\Let$-binding translates to a $\Let$-binding, a command
$\braket{\tm|\vect[n]{\frm_n}|\ko}$ translates to evaluating expression
corresponding to the term $\tm$ inside the evaluation context corresponding to
the frames $\vect[n]{\frm_n}$ and final continuation $\ko$, and a jump
$\jump{\vect{\tm}}{j}$ translates to a tail function call (more on this later).
The evaluation contexts corresponding to a single frame are given by
$Core\trans{\frm}$:
\begin{align*}
  Core\trans{\appto{\tm}} &= \hole ~ Core\trans{\tm}
  \\
  Core \trans{\castby{\cn}} &= \coerce{\hole}{\cn}
\end{align*}
The evaluation contexts corresponding to final continuations which return their
result are given by $Core\trans{\ko}_{r}$:
\begin{align*}
  Core\trans{\ret r} &= \hole
  % \\
  % Core\trans{\app{\tm}{\ko}}_{r}
  % &=
  % Core\trans{\ko}_{r}[\hole ~ Core\trans{\tm}]
  % \\
  % Core\trans{\koerce{\cn}{\ko}}_{r}
  % &=
  % Core\trans{\ko}_{r}[\coerce{\hole}{\cn}]
  \\
  Core\trans{\caseas{\ann x \ty}{\vect[n]{\analt_n}}}
  &=
  \Case \hole \As \ann x \ty \Of \vect[n]{Core\trans{\analt_n}}
\end{align*}

Besides translating terms, commands, and continuations, we also need to convert
Sequent Core bindings and alternatives back into Core bindings and alternatives.
The alternatives of case analysis have a straightforward, one-to-one
correspondence between Core and Sequent Core, given by $Core\trans{\analt}$,
where Sequent Core alternatives lead to commands that output their result on
$\ret r$:
\begin{align*}
  Core\trans{\blank \to \cmd}%_{r}
  &=
  \blank \to Core\trans{\cmd}%_{r}
  \\
  Core\trans{K ~ \vect[n]{\ann x \ty} \to \cmd}%_{r}
  &=
  K ~ \vect[n]{\ann x \ty} \to Core\trans{\cmd}%_{r}
  \\
  Core\trans{\lit \to \cmd}%_{r}
  &=
  \lit \to Core\trans{\cmd}%_{r}
\end{align*}
Translating the bindings back to Core is more interesting, since Sequent Core
has two different types of bindings (value and continuation bindings) whereas
Core only has the one.  Thus, we collapse both bound terms and continuations
into bound Core expressions, given by $Core\trans{\abind}$ and
$Core\trans{\bp}$.
\begin{align*}
  Core\trans{\nonrec{\bp}}
  &=
  \nonrec{Core\trans{\bp}}
  \\
  Core\trans{\rec{\vect[n]{\bp_n}}}
  &=
  \rec{\vect[n]{Core\trans{\bp_n}}}
\end{align*}
\begin{align*}
  Core\trans{\bindv{\ann x \ty}{\tm}}
  &=
  (\ann x \ty = Core\trans{\tm})
  \\
  Core\trans{
    \bindk{\ann j \ty}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
  }
  &=
  (
    \ann{j}{Core^\neg\trans{\ty}_{\ty'}}
    =
    \vect[n]{\fn{\ann{x_n}{\ty_n}}} Core\trans{\cmd}
    )
  \\
  \Where
  \ret r &: \ty'
\end{align*}
We need to know the type of the return continuation $\ret r$ in order to convert
continuation bindings, since continuations do not just return results, but
instead output them on the continuation $\ret r$.  In the conversion of the
continuation bound to $j$ into an expression, we effectively negate the type of
$j$.  So instead of standing for a continuation that accepts multiple inputs as
an unboxed tuple, the converted $j$ stands for a function from the multiple
inputs to the ultimate result type given by the surrounding return continuation
$r$.  The negation of a parameterized continuation is given by
$Core^\neg\trans{\sty}_{\ty}$, where $\ty$ is the ultimate return type of the
continuation:
\begin{align*}
  Core^\neg\trans{\unitjmpty}_{\ty} &= \ty
  \\
  Core^\neg\trans{\prodjmpty{\ty'}{\sty}}_{\ty}
  &=
  \ty' \to Core^\neg\trans{\sty}_{\ty}
  \\
  Core^\neg\trans{\existjmpty{\ann a \ki} \sty}_{\ty}
  &=
  \forall \ann a \ki. Core^\neg\trans{\sty}_{\ty}
\end{align*}
We can read the translation $Core^\neg\trans{\sty}_{\ty}$ as a logical negation
of $\sty$ if we understand the result type $\ty$ to stand in for falsehood.  For
instance, $\unitjmpty$ is logically interpreted as ``true,'' and so its negation
is the ``false'' return type $\ty$.  The logical negation of ``there exists an
$a$ such that $\sty$ is true'' is ``for all $a$, $\sty$ is false,'' hence the
third clause.  The middle clause for converting a product type into a function
type is a bit more indirect form of negation.  However, it follows from the
familiar De Morgan's laws if we (informally) understand the function type
$\ty \to \sty$ as a form of disjunction $(\neg \ty) \vee \sty$:
\begin{align*}
  \neg(\ty \wedge \sty)
  &=
  (\neg \ty) \vee (\neg \sty)
  =
  \ty \to (\neg \sty)
\end{align*}
Hence, the negated interpretation of the product jump type,
$Core^\neg\trans{\prodjmpty{\ty'}{\sty}}_{\ty}$, is a function,
$\ty' \to Core^\neg\trans{\sty}_{\ty}$.

To translate the whole Sequent Core program back into Core, we just translate
the main command with respect to the continuation on which we expect to receive
the program's result.
\begin{align*}
  Core\trans{\vect[n]{\decl_n}; \cmd}%_{r}
  &=
  \vect[n]{\decl_n}; Core\trans{\cmd}%_{r}
\end{align*}

\section{Design discussion}
\label{sec:design-discussion}

\subsection{Join points}

One of the goals of Sequent Core is to provide a good representation for naming
join points in a program.  A \emph{join point} is a specified point in the
control flow of a program where to different possible execution paths join back
up again.  The purpose of giving a name to these join points is to avoid
excessive duplication of code and keep code size small.  Since a join point
represents the future execution path of a program, we would like to model them
as continuations, and so named join points become named continuations.

The main place where new join points are named are in the branches of a case
expression.  For example, consider the Core expression
\begin{align*}
  e &= \Case e_0 \Of \{ K_1 ~ x ~ y \to e_1; K_2 \to e_2 \}
\end{align*}
In Sequent Core, $e$ would be represented as the term $v$:
\begin{align*}
  v
  &=
  \comp{r}
    \cut
    {\comp{q}\cmd_0}
    {\Case\Of \{ K_1 ~ x ~ y \to \cmd_1; K_2 \to \cmd_2 \}}
\end{align*}
where the term $\comp{q}\cmd_0$ corresponds to the expression $e_0$, and the
commands $\cmd_1$ and $\cmd_2$ correspond to the expressions $e_1$ and $e_2$ run
in the context of the continuation $r$ which expects the end result returned by
$e$.  Now, it would be semantically correct to inline the case continuation for
$q$ inside of $\cmd_0$, which could open up further simplification.  For
example, suppose that $\cmd_0$ is:
\begin{align*}
  \cmd_0
  &=
  \cut
  {z}
  {\Case\Of \{
    True \to \braket{K_1 | \appto{5} \then \appto{10} | \ret q};
    False \to \cut{z'}{\ret q}
  \}}
\end{align*}
Inlining for $q$ everywhere in $\cmd_0$ then corresponds to the case-of-case
transformation performed by GHC on Core expressions.

However, inlining for $q$ everywhere can duplicate the commands $\cmd_1$ and
$\cmd_2$, leading to larger code size!  We only want to inline selectively in
places where we are confident that inlining the case continuation would lead to
further simplification, but not elsewhere.  We can achieve selective inlining by
assigning the continuation to its name, $q$:
\begin{align*}
  v
  &=
  \comp{r}
  \Let q = \Case\Of \{ K_1 ~ x ~ y \to \cmd_1; K_2 \to \cmd_2 \}
  \In \cmd_0
\end{align*}
and then follow heuristics to inline for $q$ only in those places that matter.
In particular, we could just inline inside of the one branch in $\cmd_0$ which
provides $q$ with a constructed result, leading to the command:
\begin{align*}
  \cmd_0
  &=
  \cut{z}{\Case\Of \{ True \to \cmd_1[5/x,10/y]; False \to \cut{z'}{q} \}}
\end{align*}
In effect, selective inlining has eliminated case analysis on a known case by
duplicating the command $\cmd_1$, but not duplicating $\cmd_2$.

There is still a problem, though: what if the command $\cmd_1$ is so large that
this simplification is not worth the duplication?  In that case, we would like
to form a \emph{new} join point that gives a name to the command $\cmd_1$, so
that we may refer to it by name rather than duplicating the entire command.
Since $\cmd_1$ is an open command referring to local variables introduced by the
case analysis, we must first abstract over these local variables.  This is the
role of the polyadic form of continuation, which is able to receive multiple
inputs before running.  In our example, we would move $\cmd_1$ and $\cmd_2$ into
a named, polyadic continuations:
\begin{gather*}
v = \comp{r}
\begin{aligned}[t]
  &
  \Let
  \begin{aligned}[t]
    j_1 &= \fnk{x ~ y} \cmd_1
    \\
    j_2 &= \fnk{} \cmd_2
    \\
    q &= \Case\Of \{
    K_1 ~ x ~ y \to \jump{x ~ y}{j_1};
    K_2 \to \jump{}{j_2}
    \} 
  \end{aligned}
  \\
  &
  \In \cmd_0
\end{aligned}
\end{gather*}
Now, $q$ stands for a small continuation, and so it can be inlined much more
aggressively.  For example, we can perform a more lightweight inlining into
$\cmd_0$:
\begin{align*}
  \cmd_0
  &=
  \cut{z}{\Case\Of \{ True \to \jump{5 ~ 10}{j_1}; False \to \cut{z'}{\ret q} \}}
\end{align*}
which avoids duplicating $\cmd_1$ at all.

\subsection{Parameterized continuations and the existential stack}

Due to the fact that some constructors to data types contain existentially
quantified types, it is important that we be able to abstract over a command
with free type variables as a polymorphic continuation.  Thus, the more general
form of the $\lambda$-continuation may introduce several type variables in
addition to term variables.  This means that the stacks which are provided to
these continuations are effectively a form of existential tuple.  Currently, we
represent these polymorphic continuations and continuations expecting multiple
inputs as an uncurried $\lambda$-abstraction.  However, there are multiple other
design choices which are possible for this purpose:
\begin{itemize}
\item Unboxed tuples: Morally, the call stack stack
  $\appto{1}\then\appto{2}\then\appto{3}$ dual to the unboxed tuple
  $(\# 1, 2, 3 \#)$.  Why then do we bother to introduce a new form of term, and
  a new type, instead of just using unboxed tuples for this purpose?  The reason
  is the above-mentioned existentially-quantified types, which lie outside the
  form of unboxed tuples supported by Core.  We cannot represent the stack
  $\inst{Int}\then\appto{1}\then\appto{2}\then\appto{3}$ as an unboxed tuple.

  As a minor advantage, having the special form for existential stacks in a
  jumping command lets us (actually, requires us to) translate them back into
  Core as a calling context rather than a value, so $\jump{1 ~ 2 ~ 3}{j}$
  becomes $j ~ 1 ~ 2 ~ 3$ instead of $j ~ (\# 1, 2, 3 \#)$.  This results in Core
  expressions for join points looking closer to what the current GHC simplifier
  actually produces.

\item Curried vs uncurried: Currently, the continuations which accept
  existential stacks are written in a totally-uncurried form.  By listing all
  the parameters that abstract over a command, we are forced to spell out
  \emph{exactly} the number of inputs that are required by the continuation
  before any work can be done.

  An alternative design would be to give a curried form of continuations which
  accept existential stacks.  These would have one of two forms:
  \begin{itemize}
  \item $\fnk{\ann x \ty}k$: accept the first element of the stack, which is a
    term of type $\ty$, as $x$, then pass the rest of the stack to $k$ (i.e.,
    pop the top element of the stack as $x$ and then continue as $k$)
  \item $\fnk{\ann a \ki}k$: accept the first element of the stack, which is a
    type of kind $\ki$, as $a$, and then pass the rest of the stack to $k$
    (i.e., specialize the type variable $a$ for the continuation $k$)
  \end{itemize}
  which could be collapsed like ordinary $\lambda$-abstractions.  To model a
  unary continuation, like the continuation which accepts the ``end'' of the
  stack, we could introduce the dual to general computation abstractions.  These
  are continuations of the form $\letin{\ann x \ty}\cmd$ which accept an input
  named $x$ before performing some arbitrary computation $\cmd$, and correspond
  to the context $\Let x:\ty = \hole \In \expr$ in Core.  However, these
  fundamentally introduce \emph{non-strict} continuations, which is a whole can
  of worms we have been avoiding thus far.  In the end, it may be worthwhile to
  introduce these general input continuations for independent reasons, but we
  leave them out for now.

\item Case alternative: Instead of introducing a special new form of
  continuations for accepting existential stacks, we could also have added a
  form of case alternative for matching on the stack.  That way the special
  continuations $\fnk{\vect{\ann a \ki} ~ \vect{\ann x \ty}}\cmd$ would just be
  written as another form of case continuation
  $\Case\Of{\vect{\ann a \ki} ~ \vect{\ann x \ty} \to \cmd}$.  This unification
  might make sense following the story that existential stacks are just a sort
  of unboxed data type that are not available in Core, so it makes sense that we
  would pattern match on them.

  For now we don't fold continuations for existential stacks, but it is unclear
  if doing so would be a benefit (one less type of continuation to consider) or
  a hinderance (if they need to be treated differently in enough of the compiler
  that we effectively special case them anyways).  It is worth keeping this
  alternative design in mind, to evaluate which of the two options would pan out
  better in practice.
\end{itemize}

\subsection{General continuation bindings}

As part of the syntactically-enforced separation between ordinary continuations
and parameterized ones, we removed the ability to $\Let$-bind ordinary
continuations.  In practice, this means that bare case continuations and call
stacks cannot be named and inlined selectively: given a case continuation
$\Case\Of\alt{K_1 ~ x ~ y}{\cmd_1}{K_2}{\cmd_2}$, we must either inline it
everywhere, or nowhere.

The all-or-nothing inlining of ordinary continuations is ameliorated by the fact
that they can always be made ``small'' using parameterized continuations.  For
example, we always shrink the case continuation appearing in
\begin{align*}
  \cut{\comp{q}\cmd_0}{\Case\Of\alt{K_1 ~ x ~ y}{\cmd_1};\alt{K_2}{\cmd_2}}
\end{align*}
by moving the bodies of its alternatives into $\Let$-bound parameterized
continuations
\begin{align*}
  &\Let
    \begin{aligned}[t]
      \Cont j_1 &= \fnk{x ~ y} \cmd_1
      \\
      \Cont j_2 &= \fnk{} \cmd_2
    \end{aligned}
  \\
  &\In
    \cut{\comp{q}\cmd_0}{\Case\Of\alt{K_1 ~ x ~ y}{\jump{x~y}{j_1}};\alt{K_2}{\jump{}{j_2}}}
\end{align*}
and then inline for $q$ everywhere in $\cmd_0$.  Similarly, we can always shrink
the call stack appearing in
\begin{align*}
  \braket{\comp{q}\cmd_0 | \appto{\tm_1} \then \appto{\tm_2} | \ret r}
\end{align*}
by moving the arguments into $\Let$-bindings
\begin{align*}
  &\Let
    \begin{aligned}[t]
      \Val x_1 &= \tm_1
      \\
      \Val x_2 &= \tm_2
    \end{aligned}
  \\
  &\In
    \braket{\comp{q}\cmd_0 | \appto{x_1} \then \appto{x_2} | \ret q}
\end{align*}
Thus, every combination of frame stack and final continuation can be shrunk into
something small enough to be duplicated without significantly increasing the
code size.

A bit more fine-grained of a solution is to wrap up the entire continuation
inside of a $\Let$-bound, parameterized continuation.  That way, the case
statement and call stack themselves can be selectively inlined, perhaps using
the shrinking process described above as well.  To replace the original
continuation in the command, we would need to use the strict input continuation
form, $\letin{x}\cmd$, which just immediately delegates to the parameterized
continuation.  So we could turn the command
\begin{align*}
  \braket{\comp{q}\cmd_0 | \vect[n]{\frm_n} | \ko}
\end{align*}
into one where $\vect[n]{\frm_n} \then \ko$ is given a name,
\begin{align*}
  &\Let
    \begin{aligned}[t]
      \Cont j &= \fnk{x}\braket{x | \vect[n]{\frm_n} | \ko}
    \end{aligned}
  \\
  &\In
    \cut{\comp{q}\cmd_0}{\letin{x}\jump{x}{j}}
\end{align*}
This double-wrapping does introduce some new indirections, though, which could
be undesirable for rewriting.

\subsection{Continuation-closed terms}

Currently, we make the scope and type restriction that forces all terms to never
reference free continuation variables.  This kind of restriction is necessary to
ensure that Sequent Core programs correspond directly to Core programs, and do
not introduce new kinds of control flow that would require a full
continuation-passin style (CPS) translation back into Core.  However, it is
probably sufficient to impose this scoping restriction on \emph{values} (and in
particular, $\lambda$-abstractions).  This would allow for general computation
terms ($\mu$-abstractions $\comp{q}\cmd$) to reference continuations in its
scope in order to produce a (continuation-closed) value.  Since general
computations cannot escape their scope, due to call-by-need semantics, this
should be fine and should not require a full CPS translation back into Core.

The scope restriction on general computation terms comes up during translation
from Core to Sequent Core.  Currently, the restriction forces the continuation
representing the context of a case expression be let-bound, rather than use a
$\mu$-abstraction.  This forces more continuations to be abstracted into the
local let bindings, which in effect introduces more named join points back in
Core world.  This is probably not bad, since we want to be introducing join
points anyway, but it is something to keep in mind.

\end{document}
